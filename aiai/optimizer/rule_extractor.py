import json
import os
import tempfile
import time
from pathlib import Path
from textwrap import dedent

from docetl.api import ClusterOp, Dataset, MapOp, Pipeline, PipelineOutput, PipelineStep, ReduceOp, UnnestOp

from aiai.utils import setup_django

cwd = Path(__file__).parent


def build_rules_pipeline(reward: str, **kwargs) -> Pipeline:
    """
    Build a pipeline for extracting rules from logs.

    Args:
        reward: The reward type to analyze ("success", "failure", etc.)
        **kwargs: Additional arguments to pass to the Pipeline constructor

    Returns:
        Pipeline: A configured pipeline instance
    """
    operations = [
        MapOp(
            name="reward_reasoning",
            type="map",
            litellm_completion_kwargs={"temperature": 1},
            prompt=dedent(
                f"""\
                {time.time()}
                <instructions>
                    You are analyzing model outputs to understand what makes a good or bad result.

                    We've given you a task log that has been classified as: {reward}.

                    Generate a hypothetical reasoning trace that leads to the classification of this result as {reward}.

                    Focus your analysis on:
                    1. The input provided to the model
                    2. The output generated by the model
                    3. The quality and relevance of the output to the input
                    4. The context of the task and what might be expected
                    5. Any specific success or failure patterns

                    First, consider what might be important for this task. Think about what the model might be
                    optimizing for.
                    Then, formulate a hypothetical reasoning trace.
                    Finally, ensure that the reasoning trace's conclusion is that the result should be classified
                    as {reward}.
                </instructions>

                <task_log>
                    {{input.text}}
                </task_log>

                <output>
                    reward_reasoning_iterations: Iterate on the reward reasoning until you are confident
                    that the reasoning is correct.
                    reward_reasoning: The final reward reasoning.
                </output>
                """
            ),
            output={
                "schema": {
                    "reward_reasoning_iterations": "list[string]",
                    "reward_reasoning": "string",
                }
            },
            optimize=False,
        ),
        MapOp(
            name="logs_to_patterns",
            type="map",
            prompt=dedent(
                f"""\
                <instructions>
                    Analyze the following task log to identify patterns that led it to be categorized as {reward}.

                    You will be given the task input and output, and the outcome classification.

                    Your job is to deeply analyze the task and to identify patterns that led to the outcome.

                    Consider:
                    - Input quality and completeness
                    - Output quality and accuracy
                    - Relevance of the output to the input
                    - Clarity and structure of the response
                    - Any specific format requirements that were met or missed
                    - Technical correctness and completeness
                    - Overall user value of the result

                    Make sure the patterns you identify are specific and detailed. We will then be clustering
                    these patterns to identify groups and then formulate rules to improve outcomes.
                </instructions>
                <task_log>
                    {{input.text}}
                </task_log>

                <reward>
                    <hypothetical_reasoning>{{input.reward_reasoning}}</hypothetical_reasoning>
                    <outcome>{{input.reward}}</outcome>
                </reward>
                """
            ),
            output={
                "schema": {
                    "patterns": "list[string]",
                }
            },
            optimize=True,
        ),
        ClusterOp(
            name="patterns_to_insights",
            type="cluster",
            embedding_keys=[
                "patterns",
            ],
            output_key="cluster_patterns",
            summary_prompt=dedent(
                f"""\
                <instructions>
                    We've identified patterns in task results that have been classified as {reward}.
                    Now, your job is to analyze and derive insights from these patterns to inform future task execution.

                    Focus on identifying what makes results successful or unsuccessful for this specific task type.
                </instructions>
                """
                + """\
                <patterns>
                    {% for input in inputs %}
                        {% if input.patterns %}
                            {{input.patterns}}
                        {% endif %}
                    {% endfor %}
                </patterns>

                <output>
                    Insights MUST BE be a list of strings. Each insight should be specific and detailed.
                    If no insight is found, return an empty list.
                </output>
                """
            ),
            summary_schema={
                "analysis": "string",
                "insights": "list[string]",
            },
            optimize=True,
        ),
        UnnestOp(
            name="unnest_patterns_from_clusters",
            type="unnest",
            unnest_key="cluster_patterns",
        ),
        UnnestOp(
            name="unnest_insights_from_patterns",
            type="unnest",
            unnest_key="cluster_patterns",
            expand_fields=["insights"],
            output_key="insights",
        ),
        ReduceOp(
            name="insights_to_rules",
            type="reduce",
            reduce_key="insights",
            prompt=dedent(
                f"""\
                <instructions>
                    You are an expert in analyzing model outputs and task performance. We have analyzed a
                    set of task logs that have been classified as {reward} and have derived insights from
                    the patterns in these logs.

                    Your job is to generate evaluation criteria that can help improve future task
                    performance. These criteria should be specific, actionable, and focused on what makes
                    this task successful.

                    Generate both rules (in always/never format) and tips for evaluation. The rules
                    should be strict requirements, while the tips should be positive factors that
                    contribute to success.

                    IMPORTANT: Only include rules in the "always" or "never" sections if they are
                    absolutely clear-cut, binary requirements. If a rule is not a strict ALWAYS or NEVER
                    requirement, or if it's more nuanced, place it in the "tips" section instead.
                </instructions>
                """
                + """\
                <insights>
                    {% for input in inputs %}
                        {% if input.insights %}
                            {{input.insights}}
                        {% endif %}
                    {% endfor %}
                </insights>

                <output>
                    Return both rules and tips for evaluating task results:

                    1. Rules (in always/never format):
                    • Always: ONLY include rules that are absolutely clear-cut requirements
                      (format as direct actions: "Include complete information")
                    • Never: ONLY include rules that are absolutely clear-cut prohibitions
                      (format as direct actions, NOT as avoidance statements: "Return incorrect information"
                      NOT "Avoid returning incorrect information")

                    IMPORTANT: Format "never" rules as positive statements of what not to do,
                    NOT negative avoidance statements.

                    CORRECT FORMAT for "never" rules:
                    - "Return invalid results"
                    - "Use misleading information"
                    - "Ignore key requirements"

                    INCORRECT FORMAT for "never" rules (DO NOT USE THESE):
                    - "Avoid returning invalid results"
                    - "Do not use misleading information"
                    - "Don't ignore key requirements"

                    2. Tips (positive factors that contribute to success):
                    Format each tip as a clear, direct statement about what to look for or consider.
                    Include ANY rule that is not a strict ALWAYS or NEVER requirement here.

                    Example tips:
                    - "Consider breaking complex tasks into smaller steps"
                    - "Provide clear explanations for complex logic"
                    - "Ensure responses are appropriately formatted"
                    - "Consider edge cases in the input"
                    - "Balance completeness with conciseness"
                </output>
                """
            ),
            output={
                "schema": {
                    "always": "list[string]",
                    "never": "list[string]",
                    "tips": "list[string]",
                }
            },
            optimize=True,
        ),
        ReduceOp(
            name="synthesize_rules",
            type="reduce",
            reduce_key="_all",
            output={
                "schema": {
                    "always": "list[string]",
                    "never": "list[string]",
                    "tips": "list[string]",
                    "evaluation_guide": "string",
                }
            },
            prompt=dedent(
                f"""\
                <instructions>
                    You are an expert in task analysis and performance evaluation. We have analyzed a
                    set of task logs that have been classified as {reward} and have derived insights from
                    the patterns in these logs, and now we have a set of evaluation criteria.

                    Your job is to synthesize these criteria into a concise set of the most important
                    evaluation rules and tips. Identify the most valuable criteria and combine them into
                    a single set.

                    For rules, limit to no more than 3 "always" rules and 3 "never" rules.
                    For tips, limit to no more than 5 tips.

                    IMPORTANT: Only include rules in the "always" or "never" sections if they are
                    absolutely clear-cut, binary requirements. If a rule is not a strict ALWAYS or NEVER
                    requirement, or if it's more nuanced, place it in the "tips" section instead.

                    Additionally, create a brief evaluation guide that explains how to use these rules
                    and tips together to evaluate task results. The guide should suggest a scoring approach
                    where tasks can partially meet criteria rather than requiring all rules to be met.
                </instructions>
                """
                + """\
                <always>
                    {% for input in inputs %}
                        {% if input.always %}
                            {{input.always}}
                        {% endif %}
                    {% endfor %}
                </always>

                <never>
                    {% for input in inputs %}
                        {% if input.never %}
                            {{input.never}}
                        {% endif %}
                    {% endfor %}
                </never>

                <tips>
                    {% for input in inputs %}
                        {% if input.tips %}
                            {{input.tips}}
                        {% endif %}
                    {% endfor %}
                </tips>

                <output>
                    Return:
                    1. Rules (in always/never format):
                    • Always: ONLY include rules that are absolutely clear-cut requirements
                      (format as direct actions: "Provide complete information")
                    • Never: ONLY include rules that are absolutely clear-cut prohibitions
                      (format as direct actions, NOT as avoidance statements: "Return invalid results"
                      NOT "Avoid returning invalid results")

                    IMPORTANT: Format "never" rules as positive statements of what not to do,
                    NOT negative avoidance statements.

                    CORRECT FORMAT for "never" rules:
                    - "Return invalid results"
                    - "Use misleading information"
                    - "Ignore key requirements"

                    INCORRECT FORMAT for "never" rules (DO NOT USE THESE):
                    - "Avoid returning invalid results"
                    - "Do not use misleading information"
                    - "Don't ignore key requirements"

                    2. Tips (positive factors that contribute to success):
                    Format each tip as a clear, direct statement about what to look for or consider.
                    Include ANY rule that is not a strict ALWAYS or NEVER requirement here.

                    Example tips:
                    - "Consider breaking complex tasks into smaller steps"
                    - "Provide clear explanations for complex logic"
                    - "Ensure responses are appropriately formatted"
                    - "Consider edge cases in the input"
                    - "Balance completeness with conciseness"

                    3. Evaluation Guide:
                    Provide a brief guide on how to use these rules and tips together to evaluate task results.
                    Suggest a scoring approach where results can partially meet criteria rather than requiring
                    all rules to be met.
                </output>
                """
            ),
        ),
    ]

    steps = [
        PipelineStep(
            name="inverse_rl_" + reward,
            input="tasks",
            operations=[
                "reward_reasoning",
                "logs_to_patterns",
                "patterns_to_insights",
                "unnest_patterns_from_clusters",
                "unnest_insights_from_patterns",
                "insights_to_rules",
                "synthesize_rules",
            ],
        ),
    ]

    return Pipeline(
        name="rule-extractor-pipeline",
        operations=operations,
        steps=steps,
        **kwargs,
    )


def extract_rules(logs, reward="success", model="gpt-4o"):
    """
    Extract rules from a collection of logs.

    Args:
        logs: A collection of log objects with input_data and output_data attributes
        reward: The reward type to analyze ("success", "failure", etc.)
        model: The model to use for the analysis

    Returns:
        dict: A dictionary containing always/never rules, tips, and an evaluation guide
    """
    # Filter and join logs into one string
    logs_str = chr(10).join(
        "inputs:\n" + str(log.input_data) + "\nOutputs:\n" + str(log.output_data)
        for log in logs
        if "prompt" in log.input_data
    )

    if not logs_str:
        return {
            "always": [],
            "never": [],
            "tips": [],
            "evaluation_guide": "No valid logs found for analysis.",
        }

    # Write that to a temp JSON (single‐record) file
    try:
        with tempfile.NamedTemporaryFile("w", suffix=".json", delete=False) as inf:
            json.dump([{"text": logs_str}], inf)
            in_path = inf.name

        # Reserve a temp file for the pipeline output
        out_fd, out_path = tempfile.mkstemp(suffix=".json")
        os.close(out_fd)

        try:
            datasets = {
                "tasks": Dataset(
                    type="file",
                    path=in_path,
                )
            }
            pipeline = build_rules_pipeline(
                reward=reward,
                datasets=datasets,
                output=PipelineOutput(type="file", path=out_path),
                default_model=model,
            )
            pipeline.run()

            # Read back your always/never/tips
            with open(out_path) as f:
                rules = json.load(f)
            return rules

        finally:
            # Clean up temp files
            if os.path.exists(in_path):
                os.remove(in_path)
            if os.path.exists(out_path):
                os.remove(out_path)
    except Exception as e:
        # Return an empty result in case of error
        return {
            "always": [],
            "never": [],
            "tips": [],
            "evaluation_guide": f"Error during rule extraction: {str(e)}",
        }


if __name__ == "__main__":
    setup_django()
    from aiai.app.models import OtelSpan

    logs = OtelSpan.objects.all()
    rules = extract_rules(logs)
    print(rules)
