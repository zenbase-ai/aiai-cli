from typing import Literal

from dotenv import load_dotenv
from pydantic import BaseModel, Field

load_dotenv()


class EvaluationResponse(BaseModel):
    classification: Literal["good", "bad"] = Field(
        ...,
        description="Classification of the output as either 'good' or 'bad' based on the criteria.",
    )
    reasoning: str = Field(
        ..., description="A brief explanation for the classification provided."
    )


def evaluate_crew_output(output_text: str, client=None) -> EvaluationResponse | None:
    system_prompt = """
    You are an evaluation assistant. Your task is to assess the quality of personalized sales emails generated by another AI system.
    The AI was tasked with reading lead data (name, company, role, specific needs/interests) and crafting concise, personalized emails highlighting how a product called Zenbase (related to LLM optimization, prompt engineering, DSPy) can help the lead.

    A "good" output should contain one or more emails that:
    1. Are addressed to a specific person by name.
    2. Reference the lead's likely role, company, or specific interests/pain points mentioned in the context (though the context isn't provided to you, look for signs of personalization).
    3. Clearly mention Zenbase and its benefits (e.g., automating prompt engineering, model optimization, DSPy).
    4. Are reasonably well-written and professional.
    5. Have a clear call to action (e.g., suggesting a demo, call, or link).

    A "bad" output might:
    1. Be generic and not personalized.
    2. Fail to mention Zenbase or its relevant benefits.
    3. Be poorly written, nonsensical, or incomplete.
    4. Not resemble sales emails at all.
    5. Be empty or contain only error messages.

    Analyze the provided text, which is the final output from the AI system. Based on the criteria above, provide a classification ("good" or "bad") and a brief reasoning for your decision.
    """

    try:
        # Use the completion method on the instructor-patched client
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {
                    "role": "user",
                    "content": f"Please evaluate the following generated output:\n\n{output_text}",
                },
            ],
            response_model=EvaluationResponse,
            temperature=0.1,
            max_retries=2,
        )
        return response

    except Exception as e:
        print(f"Error during LLM evaluation with litellm/instructor: {e}")
        return None
